<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Amazon Nova 2: Multimodal Reasoning and Generation Models</title>
    <meta name="author" content="Siddharth Choudhary">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Technical overview of Amazon Nova 2 family: Nova 2 Lite, Pro, Omni, and Sonic - multimodal reasoning models with hybrid thinking capabilities">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#127776;</text></svg>">

    <style type="text/css">
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-weight: 300;
            font-size: 17px;
            margin-left: auto;
            margin-right: auto;
            max-width: 900px;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }

        h1 {
            font-weight: 300;
            font-size: 2.2em;
            text-align: center;
            margin-bottom: 10px;
        }

        h2 {
            font-weight: 400;
            font-size: 1.5em;
            border-bottom: 2px solid #ff9900;
            padding-bottom: 5px;
            margin-top: 40px;
        }

        h3 {
            font-weight: 400;
            font-size: 1.2em;
            color: #232f3e;
            margin-top: 30px;
        }

        h4 {
            font-weight: 500;
            font-size: 1.1em;
            margin-top: 25px;
            margin-bottom: 10px;
        }

        .author {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
        }

        .author a {
            color: #ff9900;
            text-decoration: none;
        }

        a:link, a:visited {
            color: #ff9900;
            text-decoration: none;
        }

        a:hover {
            color: #ec7211;
            text-decoration: underline;
        }

        .abstract {
            background-color: #f8f9fa;
            border-left: 4px solid #ff9900;
            padding: 15px 20px;
            margin: 20px 0;
            font-style: italic;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            font-size: 0.9em;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #232f3e;
            color: white;
        }

        tr:nth-child(even) {
            background-color: #f8f9fa;
        }

        .diagram-container {
            background-color: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin: 25px 0;
            text-align: center;
        }

        .diagram-caption {
            font-size: 0.9em;
            color: #666;
            margin-top: 10px;
            font-style: italic;
        }

        .slide-box {
            background-color: #fff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .slide-box h3 {
            margin-top: 0;
            color: #232f3e;
            border-bottom: 2px solid #ff9900;
            padding-bottom: 8px;
        }

        .model-card {
            background-color: #fff;
            border: 2px solid #ff9900;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }

        .model-card h4 {
            margin-top: 0;
            color: #232f3e;
        }

        .model-meta {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 10px;
        }

        .innovation-box {
            background-color: #fff3e0;
            border-left: 4px solid #ff9900;
            padding: 10px 15px;
            margin: 10px 0;
        }

        .architecture-list {
            background-color: #f0f0f0;
            padding: 10px 15px;
            border-radius: 5px;
            margin: 10px 0;
        }

        .qa-box {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }

        .qa-box h4 {
            color: #1565c0;
            margin-top: 0;
        }

        code {
            background-color: #f0f0f0;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        .toc {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }

        .toc li {
            margin: 8px 0;
        }

        .back-link {
            margin-bottom: 20px;
        }

        svg text {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
        }

        .benchmark-highlight {
            background-color: #d4edda;
            padding: 3px 8px;
            border-radius: 3px;
            font-weight: 500;
        }

        .vs-box {
            display: flex;
            gap: 20px;
            margin: 20px 0;
        }

        .vs-column {
            flex: 1;
            padding: 15px;
            border-radius: 8px;
        }

        .nova1-col {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
        }

        .nova2-col {
            background-color: #fff3e0;
            border: 2px solid #ff9900;
        }

        @media (max-width: 600px) {
            .vs-box {
                flex-direction: column;
            }
        }

        .slide-number {
            background-color: #232f3e;
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            display: inline-block;
            margin-bottom: 10px;
        }

        .key-stat {
            font-size: 2em;
            font-weight: 400;
            color: #ff9900;
            margin: 10px 0;
        }

        .stat-label {
            font-size: 0.9em;
            color: #666;
        }
    </style>
</head>

<body>
    <div class="back-link">
        <a href="../index.html">&larr; Back to Home</a>
    </div>

    <h1>Amazon Nova 2</h1>
    <h1 style="font-size: 1.2em; font-weight: 300; margin-top: 0;">Multimodal Reasoning and Generation Models</h1>

    <div class="author">
        <a href="https://itzsid.github.io">Siddharth Choudhary</a><br>
        January 2026
    </div>

    <div class="abstract">
        Amazon Nova 2 is a family of four foundation models designed for enterprise AI: Nova 2 Lite and Pro (multimodal reasoning with hybrid thinking), Nova 2 Omni (unified multimodal I/O), and Nova 2 Sonic (real-time speech-to-speech). This presentation covers key contributions, technical innovations, architecture deep-dives, and anticipated technical questions.
    </div>

    <div class="toc">
        <strong>Contents</strong>
        <ul>
            <li><a href="#slide1">Slide 1: Nova 2 Family Overview</a></li>
            <li><a href="#slide2">Slide 2: Key Contributions</a></li>
            <li><a href="#slide3">Slide 3: Technical Innovations vs Nova 1</a></li>
            <li><a href="#slide4">Slide 4: Hybrid Reasoning Architecture</a></li>
            <li><a href="#slide5">Slide 5: Nova 2 Omni - Unified Multimodal Architecture</a></li>
            <li><a href="#slide6">Slide 6: Nova 2 Sonic - Speech-to-Speech Architecture</a></li>
            <li><a href="#slide7">Slide 7: Training Methodology</a></li>
            <li><a href="#slide8">Slide 8: Benchmark Performance</a></li>
            <li><a href="#qa">Technical Q&A</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </div>

    <!-- ==================== SLIDE 1 ==================== -->
    <div class="slide-box" id="slide1">
        <span class="slide-number">Slide 1</span>
        <h3>Nova 2 Family Overview</h3>

        <p>Amazon Nova 2 is a family of <strong>four foundation models</strong> announced at AWS re:Invent 2025, designed to meet diverse enterprise needs across reasoning, multimodal processing, and real-time conversational AI.</p>

        <div class="diagram-container">
            <svg width="800" height="320" viewBox="0 0 800 320">
                <!-- Title -->
                <text x="400" y="25" text-anchor="middle" font-weight="bold" font-size="16">Amazon Nova 2 Model Family</text>

                <!-- Nova 2 Lite -->
                <rect x="30" y="55" width="170" height="120" rx="8" fill="#fff3e0" stroke="#ff9900" stroke-width="2"/>
                <text x="115" y="80" text-anchor="middle" font-weight="bold" font-size="13">Nova 2 Lite</text>
                <text x="115" y="100" text-anchor="middle" font-size="11" fill="#666">Fast & Cost-Effective</text>
                <line x1="50" y1="110" x2="180" y2="110" stroke="#ddd"/>
                <text x="115" y="130" text-anchor="middle" font-size="10">Hybrid reasoning</text>
                <text x="115" y="148" text-anchor="middle" font-size="10">Text + Image + Video</text>
                <text x="115" y="166" text-anchor="middle" font-size="10">1M context window</text>

                <!-- Nova 2 Pro -->
                <rect x="220" y="55" width="170" height="120" rx="8" fill="#fff3e0" stroke="#ff9900" stroke-width="2"/>
                <text x="305" y="80" text-anchor="middle" font-weight="bold" font-size="13">Nova 2 Pro</text>
                <text x="305" y="100" text-anchor="middle" font-size="11" fill="#666">Balanced Performance</text>
                <line x1="240" y1="110" x2="370" y2="110" stroke="#ddd"/>
                <text x="305" y="130" text-anchor="middle" font-size="10">Advanced reasoning</text>
                <text x="305" y="148" text-anchor="middle" font-size="10">Complex planning</text>
                <text x="305" y="166" text-anchor="middle" font-size="10">Agentic workflows</text>

                <!-- Nova 2 Omni -->
                <rect x="410" y="55" width="170" height="120" rx="8" fill="#e8f5e9" stroke="#4caf50" stroke-width="2"/>
                <text x="495" y="80" text-anchor="middle" font-weight="bold" font-size="13">Nova 2 Omni</text>
                <text x="495" y="100" text-anchor="middle" font-size="11" fill="#666">Unified Multimodal</text>
                <line x1="430" y1="110" x2="560" y2="110" stroke="#ddd"/>
                <text x="495" y="130" text-anchor="middle" font-size="10">Text+Image+Video+Audio IN</text>
                <text x="495" y="148" text-anchor="middle" font-size="10">Text + Image OUT</text>
                <text x="495" y="166" text-anchor="middle" font-size="10">First unified reasoning model</text>

                <!-- Nova 2 Sonic -->
                <rect x="600" y="55" width="170" height="120" rx="8" fill="#e3f2fd" stroke="#2196f3" stroke-width="2"/>
                <text x="685" y="80" text-anchor="middle" font-weight="bold" font-size="13">Nova 2 Sonic</text>
                <text x="685" y="100" text-anchor="middle" font-size="11" fill="#666">Speech-to-Speech</text>
                <line x1="620" y1="110" x2="750" y2="110" stroke="#ddd"/>
                <text x="685" y="130" text-anchor="middle" font-size="10">Real-time conversation</text>
                <text x="685" y="148" text-anchor="middle" font-size="10">&lt;700ms latency</text>
                <text x="685" y="166" text-anchor="middle" font-size="10">22 expressive voices</text>

                <!-- Shared capabilities -->
                <rect x="150" y="200" width="500" height="100" rx="8" fill="#f5f5f5" stroke="#999" stroke-dasharray="5,5"/>
                <text x="400" y="225" text-anchor="middle" font-weight="bold" font-size="12">Shared Capabilities</text>
                <text x="250" y="250" text-anchor="middle" font-size="11">1M Token Context</text>
                <text x="400" y="250" text-anchor="middle" font-size="11">200+ Languages</text>
                <text x="550" y="250" text-anchor="middle" font-size="11">Extended Thinking</text>
                <text x="250" y="275" text-anchor="middle" font-size="11">Transformer Architecture</text>
                <text x="400" y="275" text-anchor="middle" font-size="11">RLHF Alignment</text>
                <text x="550" y="275" text-anchor="middle" font-size="11">Enterprise Ready</text>
            </svg>
            <div class="diagram-caption">Figure 1: The Nova 2 family consists of four specialized models sharing core capabilities.</div>
        </div>
    </div>

    <!-- ==================== SLIDE 2 ==================== -->
    <div class="slide-box" id="slide2">
        <span class="slide-number">Slide 2</span>
        <h3>Key Contributions</h3>

        <div class="innovation-box">
            <strong>1. First Unified Multimodal Reasoning Model (Nova 2 Omni)</strong>
            <p style="margin-bottom:0">Industry's first reasoning model that accepts text, images, video, and speech inputs while generating both text and image outputs from a single model&mdash;eliminating multi-model coordination complexity.</p>
        </div>

        <div class="innovation-box">
            <strong>2. Hybrid Reasoning with Configurable Extended Thinking</strong>
            <p style="margin-bottom:0">Developers can toggle reasoning on/off and adjust depth through low, medium, and high settings&mdash;enabling fast responses for simple tasks while engaging systematic multi-step reasoning for complex problems.</p>
        </div>

        <div class="innovation-box">
            <strong>3. Real-Time Conversational AI (Nova 2 Sonic)</strong>
            <p style="margin-bottom:0">Sub-700ms response latency with bidirectional streaming, natural turn-taking, barge-in handling, and adaptive speech&mdash;approaching true real-time conversation territory.</p>
        </div>

        <div class="innovation-box">
            <strong>4. Massive Context Window (1M Tokens)</strong>
            <p style="margin-bottom:0">Enables analysis of extensive codebases, long documents, and hours of video within a single prompt without chunking or retrieval systems.</p>
        </div>

        <div class="innovation-box">
            <strong>5. Frontier Performance at Competitive Pricing</strong>
            <p style="margin-bottom:0">Matches or exceeds GPT-5 Mini, Gemini 2.5 Flash, and Claude Haiku 4.5 on reasoning benchmarks while maintaining industry-leading price performance.</p>
        </div>
    </div>

    <!-- ==================== SLIDE 3 ==================== -->
    <div class="slide-box" id="slide3">
        <span class="slide-number">Slide 3</span>
        <h3>Technical Innovations vs Nova 1</h3>

        <div class="vs-box">
            <div class="vs-column nova1-col">
                <h4 style="margin-top:0">Nova 1 (Dec 2024)</h4>
                <ul>
                    <li>Micro, Lite, Pro, Premier tiers</li>
                    <li>Fixed inference behavior</li>
                    <li>Separate vision/text models</li>
                    <li>Standard TTS integration</li>
                    <li>128K-300K context</li>
                    <li>Basic multimodal understanding</li>
                </ul>
            </div>
            <div class="vs-column nova2-col">
                <h4 style="margin-top:0">Nova 2 (Dec 2025)</h4>
                <ul>
                    <li>Lite, Pro, Omni, Sonic specialization</li>
                    <li><strong>Hybrid reasoning control</strong> (low/med/high)</li>
                    <li><strong>Unified multimodal I/O</strong> (Omni)</li>
                    <li><strong>Native speech-to-speech</strong> (Sonic)</li>
                    <li><strong>1M token context window</strong></li>
                    <li><strong>Cross-modal reasoning</strong></li>
                </ul>
            </div>
        </div>

        <h4>Key Technical Improvements</h4>
        <table>
            <tr>
                <th>Capability</th>
                <th>Nova 1</th>
                <th>Nova 2</th>
            </tr>
            <tr>
                <td>Reasoning Control</td>
                <td>None</td>
                <td>Extended thinking (off/low/med/high)</td>
            </tr>
            <tr>
                <td>Multimodal Output</td>
                <td>Text only</td>
                <td>Text + Images (Omni)</td>
            </tr>
            <tr>
                <td>Audio Processing</td>
                <td>External ASR/TTS</td>
                <td>Native speech understanding (Omni, Sonic)</td>
            </tr>
            <tr>
                <td>Voice Latency</td>
                <td>~2-3 seconds (cascaded)</td>
                <td>&lt;700ms (unified)</td>
            </tr>
            <tr>
                <td>Image Generation</td>
                <td>Separate Canvas model</td>
                <td>Integrated with reasoning (Omni)</td>
            </tr>
            <tr>
                <td>Character Consistency</td>
                <td>Limited</td>
                <td>Cross-image character preservation</td>
            </tr>
            <tr>
                <td>Harmful Voice Rejection</td>
                <td>Baseline</td>
                <td>>96% accuracy (+61% relative improvement)</td>
            </tr>
        </table>
    </div>

    <!-- ==================== SLIDE 4 ==================== -->
    <div class="slide-box" id="slide4">
        <span class="slide-number">Slide 4</span>
        <h3>Hybrid Reasoning Architecture</h3>

        <p>Nova 2 Lite, Pro, and Omni introduce <strong>extended thinking capabilities</strong> that give developers fine-grained control over the accuracy-cost-latency trade-off.</p>

        <div class="diagram-container">
            <svg width="750" height="300" viewBox="0 0 750 300">
                <text x="375" y="25" text-anchor="middle" font-weight="bold" font-size="14">Extended Thinking: Configurable Reasoning Depth</text>

                <!-- Input -->
                <rect x="30" y="60" width="100" height="50" rx="5" fill="#fff3e0" stroke="#ff9900" stroke-width="2"/>
                <text x="80" y="90" text-anchor="middle" font-size="12">User Query</text>

                <!-- Reasoning Controller -->
                <rect x="170" y="45" width="150" height="80" rx="5" fill="#e8f5e9" stroke="#4caf50" stroke-width="2"/>
                <text x="245" y="75" text-anchor="middle" font-weight="bold" font-size="12">Reasoning</text>
                <text x="245" y="92" text-anchor="middle" font-weight="bold" font-size="12">Controller</text>
                <text x="245" y="112" text-anchor="middle" font-size="10" fill="#666">budget: off|low|med|high</text>

                <line x1="130" y1="85" x2="170" y2="85" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <!-- Three paths -->
                <!-- Fast path -->
                <line x1="320" y1="65" x2="400" y2="65" stroke="#4caf50" stroke-width="2"/>
                <rect x="400" y="45" width="120" height="40" rx="5" fill="#c8e6c9" stroke="#4caf50"/>
                <text x="460" y="70" text-anchor="middle" font-size="11">Direct Response</text>
                <text x="560" y="70" text-anchor="start" font-size="10" fill="#666">~100ms</text>

                <!-- Medium path -->
                <line x1="320" y1="85" x2="400" y2="120" stroke="#ff9800" stroke-width="2"/>
                <rect x="400" y="100" width="200" height="40" rx="5" fill="#ffe0b2" stroke="#ff9800"/>
                <text x="500" y="120" text-anchor="middle" font-size="10">[reasoningContent: REDACTED]</text>
                <text x="500" y="132" text-anchor="middle" font-size="10">Structured analysis</text>
                <text x="620" y="120" text-anchor="start" font-size="10" fill="#666">~1-3s</text>

                <!-- Deep path -->
                <line x1="320" y1="105" x2="400" y2="180" stroke="#f44336" stroke-width="2"/>
                <rect x="400" y="155" width="280" height="55" rx="5" fill="#ffcdd2" stroke="#f44336"/>
                <text x="540" y="175" text-anchor="middle" font-size="10">[reasoningContent: REDACTED]</text>
                <text x="540" y="190" text-anchor="middle" font-size="10">Step-by-step systematic exploration</text>
                <text x="540" y="205" text-anchor="middle" font-size="10">Multi-step verification</text>
                <text x="700" y="180" text-anchor="start" font-size="10" fill="#666">~5-30s</text>

                <!-- Labels -->
                <text x="360" y="55" text-anchor="middle" font-size="10" fill="#4caf50" font-weight="bold">OFF</text>
                <text x="360" y="100" text-anchor="middle" font-size="10" fill="#ff9800" font-weight="bold">LOW/MED</text>
                <text x="360" y="165" text-anchor="middle" font-size="10" fill="#f44336" font-weight="bold">HIGH</text>

                <!-- Effort Levels Box -->
                <rect x="50" y="230" width="650" height="55" rx="5" fill="#f5f5f5" stroke="#999"/>
                <text x="375" y="250" text-anchor="middle" font-size="11" font-weight="bold">Effort Level Use Cases</text>
                <text x="150" y="272" text-anchor="middle" font-size="10"><tspan fill="#4caf50" font-weight="bold">Low:</tspan> Code review, basic analysis</text>
                <text x="375" y="272" text-anchor="middle" font-size="10"><tspan fill="#ff9800" font-weight="bold">Medium:</tspan> Debugging, planning</text>
                <text x="580" y="272" text-anchor="middle" font-size="10"><tspan fill="#f44336" font-weight="bold">High:</tspan> Math proofs, system design</text>

                <defs>
                    <marker id="arrow" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <path d="M0,0 L0,6 L9,3 z" fill="#333"/>
                    </marker>
                </defs>
            </svg>
            <div class="diagram-caption">Figure 2: Extended thinking allows developers to balance accuracy, cost, and latency based on task complexity.</div>
        </div>

        <div class="architecture-list">
            <strong>Implementation Details:</strong>
            <ul>
                <li><strong>Reasoning tokens are redacted</strong> in output (displayed as [REDACTED]) but still charged</li>
                <li>Model generates dedicated <code>reasoningContent</code> blocks with step-by-step exploration</li>
                <li>Extended thinking is <strong>off by default</strong> for fast, cost-optimized responses</li>
                <li>Training methodology improvements matter more than raw parameter count&mdash;Nova 2 Lite outperforms larger Nova Pro (8.53/10) and Nova Premier (7.16/10)</li>
            </ul>
        </div>
    </div>

    <!-- ==================== SLIDE 5 ==================== -->
    <div class="slide-box" id="slide5">
        <span class="slide-number">Slide 5</span>
        <h3>Nova 2 Omni - Unified Multimodal Architecture</h3>

        <p>Nova 2 Omni is the <strong>first unified multimodal model</strong> that processes text, documents, images, video, and audio inputs and generates text and images from a single model.</p>

        <div class="diagram-container">
            <svg width="750" height="350" viewBox="0 0 750 350">
                <text x="375" y="25" text-anchor="middle" font-weight="bold" font-size="14">Nova 2 Omni: Unified Multimodal Processing</text>

                <!-- Input Modalities -->
                <text x="80" y="55" text-anchor="middle" font-size="12" font-weight="bold">Inputs</text>

                <rect x="30" y="70" width="100" height="35" rx="5" fill="#e3f2fd" stroke="#2196f3"/>
                <text x="80" y="92" text-anchor="middle" font-size="11">Text</text>

                <rect x="30" y="115" width="100" height="35" rx="5" fill="#f3e5f5" stroke="#9c27b0"/>
                <text x="80" y="137" text-anchor="middle" font-size="11">Images</text>

                <rect x="30" y="160" width="100" height="35" rx="5" fill="#fff3e0" stroke="#ff9800"/>
                <text x="80" y="182" text-anchor="middle" font-size="11">Video</text>

                <rect x="30" y="205" width="100" height="35" rx="5" fill="#e8f5e9" stroke="#4caf50"/>
                <text x="80" y="227" text-anchor="middle" font-size="11">Audio/Speech</text>

                <rect x="30" y="250" width="100" height="35" rx="5" fill="#fce4ec" stroke="#e91e63"/>
                <text x="80" y="272" text-anchor="middle" font-size="11">Documents</text>

                <!-- Arrows to encoder -->
                <line x1="130" y1="87" x2="180" y2="150" stroke="#333" stroke-width="1.5"/>
                <line x1="130" y1="132" x2="180" y2="150" stroke="#333" stroke-width="1.5"/>
                <line x1="130" y1="177" x2="180" y2="165" stroke="#333" stroke-width="1.5"/>
                <line x1="130" y1="222" x2="180" y2="175" stroke="#333" stroke-width="1.5"/>
                <line x1="130" y1="267" x2="180" y2="185" stroke="#333" stroke-width="1.5"/>

                <!-- Multimodal Encoders -->
                <rect x="180" y="105" width="120" height="110" rx="8" fill="#e8eaf6" stroke="#3f51b5" stroke-width="2"/>
                <text x="240" y="135" text-anchor="middle" font-weight="bold" font-size="11">Multimodal</text>
                <text x="240" y="152" text-anchor="middle" font-weight="bold" font-size="11">Encoders</text>
                <text x="240" y="175" text-anchor="middle" font-size="9" fill="#666">Vision encoder</text>
                <text x="240" y="190" text-anchor="middle" font-size="9" fill="#666">Audio encoder</text>
                <text x="240" y="205" text-anchor="middle" font-size="9" fill="#666">Text tokenizer</text>

                <!-- Arrow to fusion -->
                <line x1="300" y1="160" x2="340" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <!-- Unified Transformer -->
                <rect x="340" y="80" width="180" height="160" rx="8" fill="#fff3e0" stroke="#ff9900" stroke-width="3"/>
                <text x="430" y="115" text-anchor="middle" font-weight="bold" font-size="13">Unified Transformer</text>
                <text x="430" y="135" text-anchor="middle" font-size="10" fill="#666">with Extended Thinking</text>

                <rect x="360" y="150" width="140" height="30" rx="3" fill="#ffe0b2" stroke="#ff9800"/>
                <text x="430" y="170" text-anchor="middle" font-size="10">Cross-modal Attention</text>

                <rect x="360" y="190" width="140" height="30" rx="3" fill="#c8e6c9" stroke="#4caf50"/>
                <text x="430" y="210" text-anchor="middle" font-size="10">Reasoning Engine</text>

                <!-- Arrows to output -->
                <line x1="520" y1="140" x2="580" y2="120" stroke="#333" stroke-width="2"/>
                <line x1="520" y1="180" x2="580" y2="200" stroke="#333" stroke-width="2"/>

                <!-- Output Modalities -->
                <text x="660" y="55" text-anchor="middle" font-size="12" font-weight="bold">Outputs</text>

                <rect x="580" y="85" width="130" height="60" rx="5" fill="#e3f2fd" stroke="#2196f3" stroke-width="2"/>
                <text x="645" y="110" text-anchor="middle" font-size="11" font-weight="bold">Text</text>
                <text x="645" y="130" text-anchor="middle" font-size="9" fill="#666">Reasoning, answers,</text>
                <text x="645" y="142" text-anchor="middle" font-size="9" fill="#666">transcription, translation</text>

                <rect x="580" y="165" width="130" height="60" rx="5" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2"/>
                <text x="645" y="190" text-anchor="middle" font-size="11" font-weight="bold">Images</text>
                <text x="645" y="210" text-anchor="middle" font-size="9" fill="#666">Character consistency,</text>
                <text x="645" y="222" text-anchor="middle" font-size="9" fill="#666">text rendering</text>

                <!-- Key insight -->
                <rect x="280" y="275" width="300" height="55" rx="5" fill="#fff9c4" stroke="#ffc107"/>
                <text x="430" y="295" text-anchor="middle" font-size="11" font-weight="bold">Key Innovation</text>
                <text x="430" y="315" text-anchor="middle" font-size="10">Single model replaces ASR + LLM + TTS + Image Gen pipeline</text>
            </svg>
            <div class="diagram-caption">Figure 3: Nova 2 Omni processes all input modalities through unified encoders and generates both text and images.</div>
        </div>

        <div class="architecture-list">
            <strong>Capabilities:</strong>
            <ul>
                <li><strong>Multi-speaker transcription</strong> and translation natively supported</li>
                <li><strong>Character consistency</strong> across generated images</li>
                <li><strong>Text rendering</strong> within images (historically challenging for diffusion models)</li>
                <li>Evaluated on <strong>MAVERIX</strong> benchmark&mdash;novel multimodal benchmark requiring tight video-audio integration</li>
                <li>Spatial reasoning with images (<strong>RefCOCOg</strong>)</li>
            </ul>
        </div>
    </div>

    <!-- ==================== SLIDE 6 ==================== -->
    <div class="slide-box" id="slide6">
        <span class="slide-number">Slide 6</span>
        <h3>Nova 2 Sonic - Speech-to-Speech Architecture</h3>

        <p>Nova 2 Sonic is a unified speech-to-speech model that processes audio input and generates audio output directly, preserving acoustic context like tone, pace, and emotion.</p>

        <div class="diagram-container">
            <svg width="750" height="280" viewBox="0 0 750 280">
                <text x="375" y="25" text-anchor="middle" font-weight="bold" font-size="14">Nova 2 Sonic: Real-Time Speech-to-Speech</text>

                <!-- Traditional Pipeline (crossed out) -->
                <text x="375" y="55" text-anchor="middle" font-size="12" fill="#999">Traditional Pipeline (Replaced)</text>
                <rect x="80" y="65" width="80" height="35" rx="3" fill="#f5f5f5" stroke="#ccc"/>
                <text x="120" y="87" text-anchor="middle" font-size="10" fill="#999">ASR</text>
                <line x1="160" y1="82" x2="200" y2="82" stroke="#ccc" stroke-width="1.5"/>
                <rect x="200" y="65" width="80" height="35" rx="3" fill="#f5f5f5" stroke="#ccc"/>
                <text x="240" y="87" text-anchor="middle" font-size="10" fill="#999">LLM</text>
                <line x1="280" y1="82" x2="320" y2="82" stroke="#ccc" stroke-width="1.5"/>
                <rect x="320" y="65" width="80" height="35" rx="3" fill="#f5f5f5" stroke="#ccc"/>
                <text x="360" y="87" text-anchor="middle" font-size="10" fill="#999">TTS</text>
                <text x="450" y="87" text-anchor="start" font-size="10" fill="#cc0000">~2-3s latency</text>
                <!-- Strike through -->
                <line x1="70" y1="82" x2="410" y2="82" stroke="#cc0000" stroke-width="2"/>

                <!-- Nova 2 Sonic Pipeline -->
                <text x="375" y="135" text-anchor="middle" font-size="12" font-weight="bold" fill="#2196f3">Nova 2 Sonic Unified Model</text>

                <rect x="80" y="150" width="100" height="50" rx="5" fill="#e3f2fd" stroke="#2196f3" stroke-width="2"/>
                <text x="130" y="175" text-anchor="middle" font-size="11">Speech In</text>
                <text x="130" y="190" text-anchor="middle" font-size="9" fill="#666">(streaming)</text>

                <line x1="180" y1="175" x2="230" y2="175" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <rect x="230" y="145" width="280" height="60" rx="8" fill="#e8f5e9" stroke="#4caf50" stroke-width="2"/>
                <text x="370" y="170" text-anchor="middle" font-weight="bold" font-size="12">Unified Speech LLM</text>
                <text x="370" y="188" text-anchor="middle" font-size="10" fill="#666">Bidirectional streaming + reasoning</text>

                <line x1="510" y1="175" x2="560" y2="175" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <rect x="560" y="150" width="100" height="50" rx="5" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2"/>
                <text x="610" y="175" text-anchor="middle" font-size="11">Speech Out</text>
                <text x="610" y="190" text-anchor="middle" font-size="9" fill="#666">(streaming)</text>

                <text x="690" y="175" text-anchor="start" font-size="10" fill="#4caf50" font-weight="bold">&lt;700ms</text>

                <!-- Features -->
                <rect x="100" y="225" width="140" height="40" rx="5" fill="#fff3e0" stroke="#ff9900"/>
                <text x="170" y="245" text-anchor="middle" font-size="10" font-weight="bold">Turn-Taking</text>
                <text x="170" y="258" text-anchor="middle" font-size="9" fill="#666">Detects pauses, hesitations</text>

                <rect x="260" y="225" width="140" height="40" rx="5" fill="#fff3e0" stroke="#ff9900"/>
                <text x="330" y="245" text-anchor="middle" font-size="10" font-weight="bold">Barge-In</text>
                <text x="330" y="258" text-anchor="middle" font-size="9" fill="#666">Handles interruptions</text>

                <rect x="420" y="225" width="140" height="40" rx="5" fill="#fff3e0" stroke="#ff9900"/>
                <text x="490" y="245" text-anchor="middle" font-size="10" font-weight="bold">Adaptive Speech</text>
                <text x="490" y="258" text-anchor="middle" font-size="9" fill="#666">Adjusts tone and pace</text>

                <rect x="580" y="225" width="140" height="40" rx="5" fill="#fff3e0" stroke="#ff9900"/>
                <text x="650" y="245" text-anchor="middle" font-size="10" font-weight="bold">22 Voices</text>
                <text x="650" y="258" text-anchor="middle" font-size="9" fill="#666">Polyglot expressivity</text>
            </svg>
            <div class="diagram-caption">Figure 4: Nova 2 Sonic eliminates the traditional ASR→LLM→TTS pipeline with a unified model achieving sub-700ms latency.</div>
        </div>

        <div class="architecture-list">
            <strong>Nova 2 Sonic Improvements over Nova 1 Sonic:</strong>
            <ul>
                <li><strong>Expanded language support:</strong> Portuguese and Hindi added</li>
                <li><strong>Polyglot voices:</strong> Same voice speaks different languages with native expressivity</li>
                <li><strong>Turn-taking controllability:</strong> Low, medium, or high pause sensitivity settings</li>
                <li><strong>Cross-modal interaction:</strong> Seamless switch between voice and text in same session</li>
                <li><strong>Asynchronous tool calling:</strong> Multi-step tasks without interrupting conversation</li>
                <li><strong>1M token context:</strong> Sustained interactions with full conversation history</li>
                <li><strong>Harmful voice rejection:</strong> >96% accuracy (61% relative improvement over Nova 1)</li>
            </ul>
        </div>
    </div>

    <!-- ==================== SLIDE 7 ==================== -->
    <div class="slide-box" id="slide7">
        <span class="slide-number">Slide 7</span>
        <h3>Training Methodology</h3>

        <div class="diagram-container">
            <svg width="750" height="260" viewBox="0 0 750 260">
                <text x="375" y="25" text-anchor="middle" font-weight="bold" font-size="14">Nova 2 Training Pipeline</text>

                <!-- Stage 1 -->
                <rect x="50" y="50" width="150" height="80" rx="8" fill="#e3f2fd" stroke="#2196f3" stroke-width="2"/>
                <text x="125" y="75" text-anchor="middle" font-weight="bold" font-size="11">Stage 1: Pretraining</text>
                <text x="125" y="95" text-anchor="middle" font-size="10">Licensed data</text>
                <text x="125" y="110" text-anchor="middle" font-size="10">Open source data</text>
                <text x="125" y="125" text-anchor="middle" font-size="10">Public web data</text>

                <line x1="200" y1="90" x2="230" y2="90" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <!-- Stage 2 -->
                <rect x="230" y="50" width="150" height="80" rx="8" fill="#e8f5e9" stroke="#4caf50" stroke-width="2"/>
                <text x="305" y="75" text-anchor="middle" font-weight="bold" font-size="11">Stage 2: SFT</text>
                <text x="305" y="95" text-anchor="middle" font-size="10">Supervised Fine-Tuning</text>
                <text x="305" y="110" text-anchor="middle" font-size="10">Task-specific data</text>
                <text x="305" y="125" text-anchor="middle" font-size="10">Instruction following</text>

                <line x1="380" y1="90" x2="410" y2="90" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <!-- Stage 3 -->
                <rect x="410" y="50" width="150" height="80" rx="8" fill="#fff3e0" stroke="#ff9900" stroke-width="2"/>
                <text x="485" y="75" text-anchor="middle" font-weight="bold" font-size="11">Stage 3: RLHF</text>
                <text x="485" y="95" text-anchor="middle" font-size="10">Human preferences</text>
                <text x="485" y="110" text-anchor="middle" font-size="10">RAI reward model</text>
                <text x="485" y="125" text-anchor="middle" font-size="10">DPO + PPO</text>

                <line x1="560" y1="90" x2="590" y2="90" stroke="#333" stroke-width="2" marker-end="url(#arrow)"/>

                <!-- Final -->
                <rect x="590" y="50" width="120" height="80" rx="8" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2"/>
                <text x="650" y="85" text-anchor="middle" font-weight="bold" font-size="11">Production</text>
                <text x="650" y="105" text-anchor="middle" font-size="11">Model</text>

                <!-- RAI Dimensions -->
                <rect x="100" y="160" width="550" height="80" rx="8" fill="#fafafa" stroke="#999"/>
                <text x="375" y="185" text-anchor="middle" font-weight="bold" font-size="12">Responsible AI Dimensions (Curated Data + RLHF)</text>

                <text x="180" y="210" text-anchor="middle" font-size="10" fill="#333">Safety</text>
                <text x="280" y="210" text-anchor="middle" font-size="10" fill="#333">Fairness</text>
                <text x="380" y="210" text-anchor="middle" font-size="10" fill="#333">Veracity</text>
                <text x="480" y="210" text-anchor="middle" font-size="10" fill="#333">Controllability</text>
                <text x="575" y="210" text-anchor="middle" font-size="10" fill="#333">Privacy</text>

                <text x="180" y="228" text-anchor="middle" font-size="9" fill="#666">&amp; Robustness</text>
                <text x="575" y="228" text-anchor="middle" font-size="9" fill="#666">&amp; Security</text>
            </svg>
            <div class="diagram-caption">Figure 5: Nova 2 training combines curated pretraining, supervised fine-tuning, and RLHF with responsible AI-specific reward models.</div>
        </div>

        <h4>Nova Forge: Custom Model Training</h4>
        <div class="innovation-box">
            <p style="margin:0">AWS announced <strong>Nova Forge</strong>&mdash;an "open training" paradigm enabling enterprises to build custom frontier AI models:</p>
            <ul style="margin-bottom:0">
                <li><strong>RL Gyms:</strong> Synthetic environments for domain-specific reinforcement learning</li>
                <li><strong>Synthetic Data Distillation:</strong> Smaller, faster models trained on AI-generated examples</li>
                <li><strong>Mid-training:</strong> Real-world and synthetic user-system interaction traces</li>
                <li><strong>Starting at ~$100K/year</strong> + usage-based compute through SageMaker</li>
            </ul>
        </div>
    </div>

    <!-- ==================== SLIDE 8 ==================== -->
    <div class="slide-box" id="slide8">
        <span class="slide-number">Slide 8</span>
        <h3>Benchmark Performance</h3>

        <h4>Nova 2 Lite (Frontier-Lite Tier)</h4>
        <table>
            <tr>
                <th>Benchmark</th>
                <th>Nova 2 Lite</th>
                <th>GPT-5 Mini</th>
                <th>Gemini 2.5 Flash</th>
                <th>Claude Haiku 4.5</th>
            </tr>
            <tr>
                <td>MMLU-Pro</td>
                <td>~80.9</td>
                <td>~82</td>
                <td>~82</td>
                <td>&lt;80</td>
            </tr>
            <tr>
                <td>GPQA Diamond</td>
                <td class="benchmark-highlight">~79.6</td>
                <td>~81</td>
                <td>~81</td>
                <td>&lt;75</td>
            </tr>
            <tr>
                <td>AIME 2025</td>
                <td class="benchmark-highlight">Exceeds</td>
                <td>Baseline</td>
                <td>Baseline</td>
                <td>Significantly lower</td>
            </tr>
            <tr>
                <td>MultiChallenge IF</td>
                <td class="benchmark-highlight">76.6%</td>
                <td>—</td>
                <td>—</td>
                <td>—</td>
            </tr>
        </table>

        <h4>Nova 2 Pro (Flagship Tier)</h4>
        <table>
            <tr>
                <th>Benchmark</th>
                <th>Nova 2 Pro</th>
                <th>Competitors</th>
            </tr>
            <tr>
                <td>AIME 2025</td>
                <td class="benchmark-highlight">92.3%</td>
                <td>Matches/exceeds GPT-5, Claude Sonnet 4.5, Gemini 2.5/3 Pro</td>
            </tr>
            <tr>
                <td>Long-document analysis</td>
                <td class="benchmark-highlight">Strong</td>
                <td>1M token context advantage</td>
            </tr>
            <tr>
                <td>Complex instruction following</td>
                <td class="benchmark-highlight">Strong</td>
                <td>Extended thinking enables systematic reasoning</td>
            </tr>
            <tr>
                <td>Agentic/SE tasks</td>
                <td class="benchmark-highlight">Strong</td>
                <td>Multi-step planning and workflow coordination</td>
            </tr>
        </table>

        <h4>Nova 2 Sonic (Speech-to-Speech)</h4>
        <table>
            <tr>
                <th>Metric</th>
                <th>Nova 2 Sonic</th>
                <th>GPT-4o</th>
                <th>Gemini Flash 2.0</th>
            </tr>
            <tr>
                <td>Response Latency</td>
                <td class="benchmark-highlight">&lt;700ms</td>
                <td>&gt;1s</td>
                <td>&gt;1s</td>
            </tr>
            <tr>
                <td>Harmful Voice Rejection</td>
                <td class="benchmark-highlight">>96%</td>
                <td>—</td>
                <td>—</td>
            </tr>
        </table>
    </div>

    <!-- ==================== TECHNICAL Q&A ==================== -->
    <h2 id="qa">Technical Q&A</h2>

    <div class="qa-box">
        <h4>Q1: How does multimodal fusion work?</h4>
        <p><strong>Answer:</strong> Nova 2 uses a unified Transformer architecture with modality-specific encoders that project inputs into a shared embedding space:</p>
        <ul>
            <li><strong>Vision:</strong> Images and video frames are processed through a vision encoder (likely ViT-based) producing patch embeddings</li>
            <li><strong>Audio:</strong> Speech is processed through an audio encoder, producing acoustic embeddings</li>
            <li><strong>Text:</strong> Standard tokenization and embedding lookup</li>
            <li><strong>Fusion Strategy:</strong> All modality embeddings are concatenated/interleaved and processed through cross-modal attention layers in the unified Transformer. This is closer to "early fusion" where modalities interact from the early layers, rather than late fusion where separate encoders produce final representations that are combined.</li>
            <li><strong>Output Heads:</strong> Separate heads for text generation and image generation (Nova 2 Omni), allowing the model to route to appropriate output modality.</li>
        </ul>
        <p>The 1M token context window allows all modalities to attend to each other across very long sequences, enabling reasoning that requires cross-referencing video frames, audio segments, and text.</p>
    </div>

    <div class="qa-box">
        <h4>Q2: What's the training data composition?</h4>
        <p><strong>Answer:</strong> Based on the Nova technical reports, training data includes:</p>
        <ul>
            <li><strong>Licensed and proprietary data:</strong> High-quality curated datasets from data providers</li>
            <li><strong>Open source datasets:</strong> Publicly available research datasets (CommonCrawl, Wikipedia, arXiv, etc.)</li>
            <li><strong>Publicly available data:</strong> Web data with appropriate filtering</li>
            <li><strong>Synthetic data:</strong> Nova Forge enables synthetic data-based distillation for specialized models</li>
            <li><strong>Human preference data:</strong> Collected specifically for RLHF training with RAI-specific reward models</li>
        </ul>
        <p>The data is curated along five dimensions: Safety, Fairness, Veracity & Robustness, Controllability, and Privacy & Security. The exact proportions are not publicly disclosed, but the emphasis is on quality over quantity&mdash;as demonstrated by Nova 2 Lite outperforming larger models through better training methodology.</p>
    </div>

    <div class="qa-box">
        <h4>Q3: How do you handle modality alignment?</h4>
        <p><strong>Answer:</strong> Modality alignment in Nova 2 is achieved through multiple mechanisms:</p>
        <ul>
            <li><strong>Shared embedding space:</strong> All modality encoders project to a common dimensionality, allowing the unified Transformer to process them uniformly</li>
            <li><strong>Cross-modal attention:</strong> Transformer layers learn to attend across modality boundaries, discovering correspondences (e.g., spoken words matching on-screen text)</li>
            <li><strong>Temporal alignment:</strong> For audio-video, the model learns to align acoustic events with visual events. The MAVERIX benchmark specifically tests this tight integration.</li>
            <li><strong>Contrastive pretraining:</strong> Likely uses contrastive losses (similar to CLIP) during pretraining to align visual and textual representations</li>
            <li><strong>Instruction tuning:</strong> Cross-modal instruction following (e.g., "describe what you hear at 0:45 while showing this frame") teaches explicit alignment</li>
        </ul>
        <p>For Nova 2 Sonic specifically, the speech-to-speech architecture preserves paralinguistic alignment (tone, pace, emotion) that would be lost in a cascaded ASR→LLM→TTS pipeline.</p>
    </div>

    <div class="qa-box">
        <h4>Q4: What were the key scaling challenges?</h4>
        <p><strong>Answer:</strong> Based on publicly available information and general multimodal scaling research, key challenges include:</p>
        <ul>
            <li><strong>Context window scaling:</strong> Achieving 1M tokens requires innovations in attention mechanisms (likely sparse attention, linear attention variants, or hierarchical approaches) to manage O(n²) complexity</li>
            <li><strong>Modality imbalance:</strong> Training on mixed modalities can lead to some modalities dominating. Careful curriculum design and loss weighting are required.</li>
            <li><strong>Extended thinking compute:</strong> The hybrid reasoning system must balance reasoning depth against inference cost. Training models to know when to engage deeper reasoning is non-trivial.</li>
            <li><strong>Real-time constraints:</strong> Nova 2 Sonic's &lt;700ms latency requires streaming architectures, efficient audio codecs, and potentially speculative decoding</li>
            <li><strong>Cross-modal generation:</strong> Generating images from reasoning (Nova 2 Omni) requires tight integration between the language model and diffusion components, with challenges in maintaining character consistency and accurate text rendering</li>
            <li><strong>Responsible AI at scale:</strong> Ensuring safety, fairness, and robustness across all modalities and languages (200+) requires extensive red-teaming and curated training data</li>
        </ul>
        <p>The Nova Forge announcement suggests AWS is addressing the "frontier-scale data" challenge by enabling customers to contribute their own data for mid-training and continued pretraining, rather than relying solely on publicly available data.</p>
    </div>

    <!-- ==================== REFERENCES ==================== -->
    <h2 id="references">References</h2>

    <ol>
        <li>Amazon. "Amazon Nova 2: Multimodal Reasoning and Generation Models." Technical Report, December 2025. <a href="https://www.amazon.science/publications/amazon-nova-2-multimodal-reasoning-and-generation-models">Amazon Science</a></li>
        <li>Amazon. "The Amazon Nova Family of Models: Technical Report and Model Card." December 2024 (updated March 2025). <a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">Amazon Science</a></li>
        <li>AWS. "Introducing Amazon Nova 2 Lite: A fast, cost-effective reasoning model." AWS Blog, December 2025. <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-nova-2-lite-a-fast-cost-effective-reasoning-model/">AWS Blog</a></li>
        <li>AWS. "Introducing Amazon Nova 2 Sonic: Our new speech-to-speech model for conversational AI." AWS Blog, December 2025. <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-nova-2-sonic-next-generation-speech-to-speech-model-for-conversational-ai/">AWS Blog</a></li>
        <li>AWS. "Extended thinking in Amazon Nova 2." Documentation. <a href="https://docs.aws.amazon.com/nova/latest/nova2-userguide/extended-thinking.html">AWS Docs</a></li>
        <li>Amazon. "Amazon Nova Forge: Open training paradigm." Amazon Science Blog, December 2025. <a href="https://www.amazon.science/blog/amazon-nova-forge-open-training-paradigm-that-empowers-everyone-to-build-their-own-frontier-ai">Amazon Science</a></li>
        <li>AWS. "Amazon Nova foundation models." Product page. <a href="https://aws.amazon.com/nova/models/">AWS Nova</a></li>
        <li>Caylent. "AWS re:Invent 2025: Every AI Announcement." December 2025. <a href="https://caylent.com/blog/aws-reinvent-2025-every-ai-announcement-including-amazon-nova-2-and-kiro">Caylent Blog</a></li>
    </ol>

    <hr style="margin-top: 40px;">
    <p style="text-align: center; color: #666; font-size: 0.9em;">
        Last updated: January 2026 &middot; <a href="https://itzsid.github.io">Siddharth Choudhary</a>
    </p>

</body>
</html>
